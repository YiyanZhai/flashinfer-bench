{
  "device_info": {
    "device_name": "NVIDIA GeForce RTX 4090",
    "device_id": 0,
    "device_str": "cuda:0",
    "compute_capability": [
      8,
      9
    ],
    "total_memory": 25386352640,
    "driver_version": "12.4",
    "backend": "triton"
  },
  "kernel_spec": {
    "description_file": "benchmark/level1/34_InstanceNorm.py",
    "generator_file": "generators/kernelllm_generator.py",
    "warmup": 5,
    "iterations": 10,
    "max_diff_limit": 1e-05,
    "report_n": 10,
    "correctness_trials": 1,
    "seed": 42,
    "backend": "triton",
    "use_ncu": false
  },
  "baseline_time": 0.8993632316589355,
  "success_rate": 0.0,
  "avg_speedup": 0.0,
  "max_speedup": 0.0,
  "compilation_success_rate": 1.0,
  "correctness_success_rate": 0.0,
  "generations": [
    {
      "round": 0,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel):\n    XBLOCK: tl.constexpr = 1\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = tl.full([1], xoffset, tl.int32)\n    tl.full([RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[:]\n    tl.full([RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])\n    tmp3 = tl.broadcast_to(tmp1, [RBLOCK])\n    tmp5 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))\n    tmp6 = tl.full([1], 256, tl.int32)\n    tmp7 = tmp6.to(tl.float32)\n    tmp8 = tmp5 / tmp7\n    tmp9 = tmp1 - tmp8\n    tmp10 = tmp9 * tmp9\n    tmp11 = tl.broadcast_to(tmp10, [RBLOCK])\n    tmp13 = triton_helpers.promote_to_tensor(tl.sum(tmp11, 0))\n    tmp14 = tmp0 - tmp8\n    tmp15 = 256.0\n    tmp16 = tmp13 / tmp15\n    tmp17 = 1e-05\n    tmp18 = tmp16 + tmp17\n    tmp19 = libdevice.rsqrt(tmp18)\n    tmp20 = tmp14 * tmp19\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp20, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (524288, 65536, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (2621440, 2560, 256, \n            1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1, buf3,\n            1024, 256, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (524288, 65536, 256,\n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==524288 at dim=0",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 1,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 16, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 16.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (524288, 65536, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (8388608, 8192, 256, \n            1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (524288, 8192, 256, \n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==524288 at dim=0",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 2,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 16, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 16.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 16, 16), (16384, 256, 16, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 16, 16), (262144, 256, 16, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 16, 16), (16384, 256, 16, 1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16384 at dim=0; expected size 64==64, stride 65536==256 at dim=1; expected size 256==16, stride 256==16 at dim=2; expected size 256==16, stride 1==1 at dim=3",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 3,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 256, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 256.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (524288, 8192, 32, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (268435456, 262144, \n            1024, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 256, XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (524288, 8192, 32, \n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==524288 at dim=0; expected size 64==64, stride 65536==8192 at dim=1; expected size 256==256, stride 256==32 at dim=2",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 4,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 16, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 16.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 16, 16), (16384, 256, 16, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 16, 16), (262144, 256, 16, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 16, 16), (16384, 256, 16, 1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16384 at dim=0; expected size 64==64, stride 65536==256 at dim=1; expected size 256==16, stride 256==16 at dim=2; expected size 256==16, stride 1==1 at dim=3",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 5,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 256, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 256.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (16777216, 256, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (2621440, 2560, 256, \n            1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 256, XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (16777216, 256, 256,\n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16777216 at dim=0; expected size 64==64, stride 65536==256 at dim=1",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 6,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 16, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 16.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 16, 16), (16384, 256, 16, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 16, 16), (262144, 256, 16, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 16, 16), (16384, 256, 16, 1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16384 at dim=0; expected size 64==64, stride 65536==256 at dim=1; expected size 256==16, stride 256==16 at dim=2; expected size 256==16, stride 1==1 at dim=3",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 7,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 256, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 256.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (16777216, 256, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (268435456, 262144, \n            256, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 256, XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (16777216, 262144, \n        256, 1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n\n    def __init__(self, num_features: 'int'):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16777216 at dim=0; expected size 64==64, stride 65536==256 at dim=1",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 8,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 256, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 256.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (16777216, 256, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (2621440, 2560, 256, \n            1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 256, XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (16777216, 256, 256,\n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16777216 at dim=0; expected size 64==64, stride 65536==256 at dim=1",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 9,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_per_fused__native_batch_norm_legit_0(in_ptr0, out_ptr2, xnumel,\n    rnumel, XBLOCK: tl.constexpr):\n    xnumel = 1024\n    RBLOCK: tl.constexpr = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 256 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tl.where(xmask, tmp1, 0)\n    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp6 = tl.where(xmask, tmp4, 0)\n    tmp7 = tl.sum(tmp6, 1)[:, None]\n    tmp8 = tl.full([XBLOCK, 1], 256, tl.int32)\n    tmp9 = tmp8.to(tl.float32)\n    tmp10 = tmp7 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp11 * tmp11\n    tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])\n    tmp15 = tl.where(xmask, tmp13, 0)\n    tmp16 = tl.sum(tmp15, 1)[:, None]\n    tmp17 = tmp0 - tmp10\n    tmp18 = 256.0\n    tmp19 = tmp16 / tmp18\n    tmp20 = 1e-05\n    tmp21 = tmp19 + tmp20\n    tmp22 = libdevice.rsqrt(tmp21)\n    tmp23 = tmp17 * tmp22\n    tl.store(out_ptr2 + (r1 + 256 * x0), tmp23, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 64, 256, 256), (16777216, 256, 256, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf3 = empty_strided_cuda((1, 1024, 256, 256), (268435456, 256, 256,\n            1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__native_batch_norm_legit_0[grid(1024)](arg0_1,\n            buf3, 1024, 256, XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return reinterpret_tensor(buf3, (16, 64, 256, 256), (16777216, 256, 256,\n        1), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "expected size 16==16, stride 4194304==16777216 at dim=0; expected size 64==64, stride 65536==256 at dim=1",
            "error_type": "AssertionError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    }
  ]
}