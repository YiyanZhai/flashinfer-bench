{
  "device_info": {
    "device_name": "NVIDIA GeForce RTX 4090",
    "device_id": 0,
    "device_str": "cuda:0",
    "compute_capability": [
      8,
      9
    ],
    "total_memory": 25386352640,
    "driver_version": "12.4",
    "backend": "triton"
  },
  "kernel_spec": {
    "description_file": "benchmark/level1/24_LogSoftmax.py",
    "generator_file": "generators/kernelllm_generator.py",
    "warmup": 5,
    "iterations": 10,
    "max_diff_limit": 1e-05,
    "report_n": 10,
    "correctness_trials": 1,
    "seed": 42,
    "backend": "triton",
    "use_ncu": false
  },
  "baseline_time": 0.00621119998395443,
  "success_rate": 0.0,
  "avg_speedup": 0.0,
  "max_speedup": 0.0,
  "compilation_success_rate": 0.1,
  "correctness_success_rate": 0.0,
  "generations": [
    {
      "round": 0,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tmp9 = tl_math.log(tmp8)\n    tmp10 = tmp4 - tmp9\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp10, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(262144)](arg0_1, buf2, 262144,\n            16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "runtime_error_name": "RuntimeError"
      },
      "compilation_error": null,
      "runtime_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "generation_error": null
    },
    {
      "round": 1,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tmp9 = tl_math.log(tmp8)\n    tmp10 = tmp4 - tmp9\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp10, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(262144)](arg0_1, buf2, 262144,\n            16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 2,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    xnumel = 16\n    RBLOCK: tl.constexpr = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16384 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(out_ptr2 + (r1 + 16384 * x0), tmp12, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(16)](arg0_1, buf2, 16, 16384,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 3,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    xnumel = 16\n    RBLOCK: tl.constexpr = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16384 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(out_ptr2 + (r1 + 16384 * x0), tmp12, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(16)](arg0_1, buf2, 16, 16384,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 4,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_red_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n    xnumel = 16\n    rnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp2 = tl.full([XBLOCK, RBLOCK], float('-inf'), tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(in_ptr0 + (r1 + 16384 * x0), rmask & xmask,\n            eviction_policy='evict_last', other=0.0)\n        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n        tmp3 = triton_helpers.maximum(_tmp2, tmp1)\n        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)\n    tmp2 = triton_helpers.max2(_tmp2, 1)[:, None]\n    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp4 = tl.load(in_ptr0 + (r1 + 16384 * x0), rmask & xmask,\n            eviction_policy='evict_last', other=0.0)\n        tmp5 = tmp4 - tmp2\n        tmp6 = tl_math.exp(tmp5)\n        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n        tmp9 = _tmp8 + tmp7\n        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)\n    tmp8 = tl.sum(_tmp8, 1)[:, None]\n    _tmp16 = tl.full([XBLOCK, RBLOCK], float('-inf'), tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp10 = tl.load(in_ptr0 + (r1 + 16384 * x0), rmask & xmask,\n            eviction_policy='evict_last', other=0.0)\n        tmp11 = tmp10 - tmp2\n        tmp12 = tl_math.log(tmp8)\n        tmp13 = tmp11 - tmp12\n        tmp14 = -tmp13\n        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])\n        tmp17 = triton_helpers.maximum(_tmp16, tmp15)\n        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)\n    tmp16 = triton_helpers.max2(_tmp16, 1)[:, None]\n    _tmp26 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp18 = tl.load(in_ptr0 + (r1 + 16384 * x0), rmask & xmask,\n            eviction_policy='evict_last', other=0.0)\n        tmp19 = tmp18 - tmp2\n        tmp20 = tl_math.log(tmp8)\n        tmp21 = tmp19 - tmp20\n        tmp22 = -tmp21\n        tmp23 = tmp22 - tmp16\n        tmp24 = tl_math.exp(tmp23)\n        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, RBLOCK])\n        tmp27 = _tmp26 + tmp25\n        _tmp26 = tl.where(rmask & xmask, tmp27, _tmp26)\n    tmp26 = tl.sum(_tmp26, 1)[:, None]\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp28 = tl.load(in_ptr0 + (r1 + 16384 * x0), rmask & xmask,\n            eviction_policy='evict_first', other=0.0)\n        tmp29 = tmp28 - tmp2\n        tmp30 = tl_math.log(tmp8)\n        tmp31 = tmp29 - tmp30\n        tmp32 = -tmp31\n        tmp33 = tmp32 - tmp16\n        tmp34 = tl_math.exp(tmp33)\n        tmp35 = tmp34 / tmp26\n        tl.store(out_ptr2 + (r1 + 16384 * x0), tmp35, rmask & xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_red_fused__log_softmax_0[grid(16)](arg0_1, buf2, 16, 16384,\n            XBLOCK=1, RBLOCK=2048, num_warps=16, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 5,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    xnumel = 16\n    RBLOCK: tl.constexpr = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16384 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(out_ptr2 + (r1 + 16384 * x0), tmp12, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(16)](arg0_1, buf2, 16, 16384,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 6,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tmp9 = tl_math.log(tmp8)\n    tmp10 = tmp4 - tmp9\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp10, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(262144)](arg0_1, buf2, 262144,\n            16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 7,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tmp9 = tl_math.log(tmp8)\n    tmp10 = tmp4 - tmp9\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp10, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(262144)](arg0_1, buf2, 262144,\n            16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: 'int'=1):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 8,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    xnumel = 16\n    RBLOCK: tl.constexpr = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16384 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(out_ptr2 + (r1 + 16384 * x0), tmp12, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(16)](arg0_1, buf2, 16, 16384,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 9,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__log_softmax_0(in_ptr0, out_ptr2, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 16 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tmp9 = tl_math.log(tmp8)\n    tmp10 = tmp4 - tmp9\n    tl.store(out_ptr2 + (r1 + 16 * x0), tmp10, None)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (16, 16384), (16384, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf2 = empty_strided_cuda((16, 16384), (16384, 1), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__log_softmax_0[grid(262144)](arg0_1, buf2, 262144,\n            16, XBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf2,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    }
  ]
}