{
    "name": "batch_mla_paged_attention",
    "type": "decode",
    "description": "Multi-head Latent Attention (MLA) with paged KV-cache as used in DeepSeek-V2/V3 models.",
    "axes": {
      "B": { "type": "var" },
      "B_plus_1": { "type": "var", "parent": "B" },
      "seq_len": { "type": "var", "parent": "B" },
      "H_qo": { "type": "const", "value": 128 },
      "D_ckv": { "type": "const", "value": 512 },
      "D_kpe": { "type": "const", "value": 64 },
      "page_size": { "type": "const", "value": 1 },
      "num_pages": { "type": "var" }
    },
    "inputs": {
      "q_nope": {
        "shape": ["B", "H_qo", "D_ckv"],
        "dtype": "bfloat16"
      },
      "q_pe": {
        "shape": ["B", "H_qo", "D_kpe"],
        "dtype": "bfloat16"
      },
      "ckv_cache": {
        "shape": ["num_pages", "page_size", "D_ckv"],
        "dtype": "bfloat16"
      },
      "kpe_cache": {
        "shape": ["num_pages", "page_size", "D_kpe"],
        "dtype": "bfloat16"
      },
      "qo_indptr": {
        "shape": ["B_plus_1"],
        "dtype": "int32"
      },
      "kv_indptr": {
        "shape": ["B_plus_1"],
        "dtype": "int32"
      },
      "kv_indices": {
        "shape": ["num_pages"],
        "dtype": "int32"
      },
      "kv_len_arr": {
        "shape": ["B"],
        "dtype": "int32"
      },
      "sm_scale": {
        "shape": [],
        "dtype": "float32"
      },
      "causal": {
        "shape": [],
        "dtype": "bool"
      }
    },
    "outputs": {
      "output": {
        "shape": ["B", "H_qo", "D_ckv"],
        "dtype": "bfloat16"
      },
      "lse": {
        "shape": ["B", "H_qo"],
        "dtype": "float32"
      }
    },
    "constraints": [
      "D_ckv == 512",
      "D_kpe == 64",
      "sm_scale == 1.0 / sqrt(D_ckv + D_kpe)"
    ],
    "reference": "import torch\nfrom flashinfer.mla import BatchMLAPagedAttentionWrapper\n\ndef run(\n    q_nope, q_pe, ckv_cache, kpe_cache,\n    qo_indptr, kv_indptr, kv_indices, kv_len_arr,\n    sm_scale, causal\n):\n    device = q_nope.device\n    batch_size, H_qo, D_ckv = q_nope.shape\n    _, _, D_kpe = q_pe.shape\n    page_size = 1  # From the constraint\n    \n    # Create workspace buffer (128MB recommended)\n    workspace_buffer = torch.empty(128 * 1024 * 1024, dtype=torch.int8, device=device)\n    \n    # Initialize MLA wrapper\n    mla_wrapper = BatchMLAPagedAttentionWrapper(\n        workspace_buffer,\n        backend=\"fa2\"\n    )\n    \n    # Plan the MLA attention computation\n    mla_wrapper.plan(\n        qo_indptr,\n        kv_indptr,\n        kv_indices,\n        kv_len_arr,\n        H_qo,\n        D_ckv,\n        D_kpe,\n        page_size,\n        causal,\n        sm_scale,\n        q_nope.dtype,\n        ckv_cache.dtype,\n    )\n    \n    # Run MLA attention\n    output, lse = mla_wrapper.run(\n        q_nope,\n        q_pe,\n        ckv_cache,\n        kpe_cache,\n        return_lse=True\n    )\n    \n    return {\"output\": output, \"lse\": lse}"
  }