{
  "device_info": {
    "device_name": "NVIDIA GeForce RTX 4090",
    "device_id": 0,
    "device_str": "cuda:0",
    "compute_capability": [
      8,
      9
    ],
    "total_memory": 25386352640,
    "driver_version": "12.4",
    "backend": "triton"
  },
  "kernel_spec": {
    "description_file": "benchmark/level1/98_KLDivLoss.py",
    "generator_file": "generators/kernelllm_generator.py",
    "warmup": 5,
    "iterations": 10,
    "max_diff_limit": 1e-05,
    "report_n": 10,
    "correctness_trials": 1,
    "seed": 42,
    "backend": "triton",
    "use_ncu": false
  },
  "baseline_time": 0.025702399015426636,
  "success_rate": 0.0,
  "avg_speedup": 0.0,
  "max_speedup": 0.0,
  "compilation_success_rate": 0.9,
  "correctness_success_rate": 0.0,
  "generations": [
    {
      "round": 0,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, out_ptr1, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp3, None)\n    tl.store(out_ptr1 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.\n    constexpr, RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex\n        tmp0 = tl.load(in_ptr0 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp1 = tl.load(in_ptr1 + 0)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = tl.load(in_ptr2 + 0)\n        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])\n        tmp16 = tl.load(in_ptr3 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp3 = tmp0 - tmp2\n        tmp6 = tmp5 - tmp2\n        tmp7 = tmp3 / tmp6\n        tmp8 = libdevice.isnan(tmp7).to(tl.int1)\n        tmp9 = 0.0\n        tmp10 = tmp7 == tmp9\n        tmp11 = tl_math.log(tmp7)\n        tmp12 = tmp7 * tmp11\n        tmp13 = tl.where(tmp10, tmp9, tmp12)\n        tmp14 = float('nan')\n        tmp15 = tl.where(tmp8, tmp14, tmp13)\n        tmp17 = tmp16 - tmp2\n        tmp18 = tmp17 / tmp6\n        tmp19 = libdevice.isnan(tmp18).to(tl.int1)\n        tmp20 = tmp18 == tmp9\n        tmp21 = tl_math.log(tmp18)\n        tmp22 = tmp18 * tmp21\n        tmp23 = tl.where(tmp20, tmp9, tmp22)\n        tmp24 = tl.where(tmp19, tmp14, tmp23)\n        tmp25 = tmp7 * tmp24\n        tmp26 = tmp15 - tmp25\n        tmp27 = tl.broadcast_to(tmp26, [XBLOCK, RBLOCK])\n        tmp29 = _tmp34 + tmp27\n        _tmp34 = tl.where(rmask, tmp29, _tmp34)\n        tmp30 = 1e-08\n        tmp31 = tmp6 + tmp30\n        tmp32 = tl_math.log(tmp31)\n        tmp33 = tmp7 * tmp32\n        tmp35 = tmp8 + tmp10\n        tmp36 = tmp35 - tmp33\n        tmp37 = tl.broadcast_to(tmp36, [XBLOCK, RBLOCK])\n        tmp39 = _tmp34 + tmp37\n        _tmp34 = tl.where(rmask, tmp39, _tmp34)\n    tmp34 = tl.sum(_tmp34, 1)[:, None]\n    tmp40 = 0.000244140625\n    tmp41 = tmp34 * tmp40\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp41, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((), (), torch.float32)\n        buf1 = empty_strided_cuda((), (), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf0, buf1, 1, 512,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg1_1\n        buf3 = empty_strided_cuda((), (), torch.float32)\n        buf4 = empty_strided_cuda((), (), torch.float32)\n        triton_per_fused__softmax_0[grid(1)](arg0_1, buf3, buf4, 1, 512,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n        buf5 = empty_strided_cuda((), (), torch.float32)\n        buf6 = buf5\n        del buf5\n        triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf6,\n            arg0_1, buf0, buf1, arg1_1, 1, 4096, XBLOCK=1, RBLOCK=2048,\n            num_warps=16, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n        del buf3\n        del buf4\n    return buf6,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'arg0_1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 1,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel):\n    XBLOCK: tl.constexpr = 1\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    tl.full([1], xoffset, tl.int32)\n    tl.full([RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[:]\n    tl.full([RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp9 = tl.load(in_ptr1 + r0, None)\n    tmp12 = tl.load(in_ptr2 + 0)\n    tmp13 = tl.broadcast_to(tmp12, [RBLOCK])\n    tmp16 = tl.load(in_out_ptr0 + 0)\n    tmp17 = tl.broadcast_to(tmp16, [1])\n    tmp1 = libdevice.isnan(tmp0).to(tl.int1)\n    tmp2 = 0.0\n    tmp3 = tmp0 == tmp2\n    tmp4 = tl_math.log(tmp0)\n    tmp5 = tmp0 * tmp4\n    tmp6 = tl.where(tmp3, tmp2, tmp5)\n    tmp7 = float('nan')\n    tmp8 = tl.where(tmp1, tmp7, tmp6)\n    tmp10 = tl_math.log(tmp9)\n    tmp11 = tmp0 * tmp10\n    tmp14 = tmp11 - tmp13\n    tmp15 = tmp8 + tmp14\n    tmp18 = 0.25\n    tmp19 = tmp17 * tmp18\n    tmp20 = tmp15 / tmp19\n    tmp21 = tl.broadcast_to(tmp20, [RBLOCK])\n    tmp23 = triton_helpers.promote_to_tensor(tl.sum(tmp21, 0))\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([1], 0, tl.int32), tmp23, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((), (), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf0, 1, 128, XBLOCK=1,\n            num_warps=2, num_stages=1)\n        buf3 = buf0\n        del buf0\n        triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf3,\n            arg0_1, arg1_1, buf1, 1, 512, num_warps=4, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf1\n    return buf3,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'buf1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 2,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.load(in_ptr1 + 0)\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp8 = tl.load(in_ptr2 + r0, None)\n    tmp9 = tl.load(in_ptr3 + 0)\n    tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n    tmp3 = tmp0 / tmp2\n    tmp4 = libdevice.isnan(tmp3).to(tl.int1)\n    tmp5 = 0.0\n    tmp6 = tmp3 == tmp5\n    tmp7 = tl_math.log(tmp3)\n    tmp11 = tmp8 / tmp10\n    tmp12 = tmp3 * tmp11\n    tmp13 = tmp3 - tmp11\n    tmp14 = tl.where(tmp6, tmp5, tmp13)\n    tmp15 = tl.where(tmp4, tmp5, tmp14)\n    tmp16 = tmp3 * tmp15\n    tmp17 = tl.broadcast_to(tmp16, [XBLOCK, RBLOCK])\n    tmp19 = tl.sum(tmp17, 1)[:, None]\n    tmp20 = 0.25\n    tmp21 = tmp19 * tmp20\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp21, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((), (), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg0_1, buf0, 1, 4096, XBLOCK=\n            1, num_warps=2, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((), (), torch.float32)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf1, 1, 4096, XBLOCK=\n            1, num_warps=2, num_stages=1)\n        del arg1_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2\n        del buf2\n        triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf3,\n            buf0, buf1, arg0_1, arg1_1, 1, 128, XBLOCK=1, num_warps=2,\n            num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n    return buf3,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'arg0_1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 3,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    _tmp68 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex\n        tmp0 = tl.load(in_ptr0 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp1 = tl.load(in_ptr1 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp10 = tl.load(in_ptr2 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp11 = tl.load(in_ptr3 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp2 = tmp0 - tmp1\n        tmp3 = tl_math.exp(tmp2)\n        tmp4 = tmp3 / tmp1\n        tmp5 = libdevice.isnan(tmp4).to(tl.int1)\n        tmp6 = 0.0\n        tmp7 = tmp4 == tmp6\n        tmp8 = tl_math.log(tmp4)\n        tmp9 = tmp4 * tmp8\n        tmp12 = tmp10 - tmp11\n        tmp13 = tl_math.exp(tmp12)\n        tmp14 = tmp13 / tmp11\n        tmp15 = libdevice.isnan(tmp14).to(tl.int1)\n        tmp16 = tmp14 == tmp6\n        tmp17 = tl_math.log(tmp14)\n        tmp18 = tmp14 * tmp17\n        tmp19 = tl.where(tmp16, tmp6, tmp18)\n        tmp20 = tl.where(tmp15, tmp6, tmp19)\n        tmp21 = tmp4 * tmp20\n        tmp22 = tmp4 - tmp20\n        tmp23 = tl_math.exp(tmp22)\n        tmp24 = tl_math.log(tmp23)\n        tmp25 = tmp22 - tmp24\n        tmp26 = tmp9 - tmp25\n        tmp27 = tl.where(tmp7, tmp6, tmp26)\n        tmp28 = tmp4 * tmp27\n        tmp29 = tmp14 * tmp9\n        tmp30 = tmp14 - tmp9\n        tmp31 = tl_math.exp(tmp30)\n        tmp32 = tl_math.log(tmp31)\n        tmp33 = tmp30 - tmp32\n        tmp35 = tl.broadcast_to(tmp34, [XBLOCK, RBLOCK])\n        tmp37 = tl.sum(tmp35, 1)[:, None]\n        tmp38 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])\n        tmp40 = tl.sum(tmp38, 1)[:, None]\n        tmp41 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])\n        tmp43 = tl.sum(tmp41, 1)[:, None]\n        tmp44 = tl.broadcast_to(tmp23, [XBLOCK, RBLOCK])\n        tmp46 = tl.sum(tmp44, 1)[:, None]\n        tmp47 = tl.broadcast_to(tmp31, [XBLOCK, RBLOCK])\n        tmp49 = tl.sum(tmp47, 1)[:, None]\n        tmp50 = 0.25\n        tmp51 = tmp37 * tmp50\n        tmp52 = tmp40 + tmp43\n        tmp53 = tmp52 + tmp46\n        tmp54 = tmp53 + tmp49\n        tmp55 = tmp54 + tmp51\n        tmp56 = tmp55 - tmp51\n        tmp57 = tmp56 + tmp51\n        tmp58 = 1e-06\n        tmp59 = tmp57 + tmp58\n        tmp60 = tmp51 / tmp59\n        tmp61 = tmp60 - tmp50\n        tmp62 = tmp61 + tmp50\n        tmp63 = tl.broadcast_to(tmp62, [XBLOCK, RBLOCK])\n        tmp65 = tl.sum(tmp63, 1)[:, None]\n        tmp66 = 4096.0\n        tmp67 = tmp65 / tmp66\n        tmp69 = tmp67 + tmp68\n        tl.debug_barrier()\n        tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp69, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((), (), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf0, 1, 128, XBLOCK=1,\n            num_warps=2, num_stages=1)\n        buf1 = empty_strided_cuda((), (), torch.float32)\n        triton_per_fused__softmax_0[grid(1)](arg0_1, buf1, 1, 128, XBLOCK=1,\n            num_warps=2, num_stages=1)\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf8 = buf2\n        del buf2\n        buf9 = buf8\n        del buf8\n        triton_red_fused__softmax_add_div_mul_sub_sum_xlogy_1[grid(1)](buf9,\n            arg1_1, arg0_1, buf0, buf1, 1, 4096, XBLOCK=1, RBLOCK=2048,\n            num_warps=16, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n    return buf9,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "at 53:32:\n        tmp23 = tl_math.exp(tmp22)\n        tmp24 = tl_math.log(tmp23)\n        tmp25 = tmp22 - tmp24\n        tmp26 = tmp9 - tmp25\n        tmp27 = tl.where(tmp7, tmp6, tmp26)\n        tmp28 = tmp4 * tmp27\n        tmp29 = tmp14 * tmp9\n        tmp30 = tmp14 - tmp9\n        tmp31 = tl_math.exp(tmp30)\n        tmp32 = tl_math.log(tmp31)\n        tmp33 = tmp30 - tmp32\n        tmp35 = tl.broadcast_to(tmp34, [XBLOCK, RBLOCK])\n                                ^\nNameError('tmp34 is not defined')",
            "error_type": "CompilationError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 4,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 128 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + x0, tmp8, None)\n\n\n@triton.jit\ndef triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel):\n    XBLOCK: tl.constexpr = 1\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    tl.full([1], xoffset, tl.int32)\n    tl.full([RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[:]\n    tl.full([RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp9 = tl.load(in_ptr1 + r0, None)\n    tmp15 = tl.load(in_ptr2 + r0, None)\n    tmp22 = tl.load(in_ptr3 + r0, None)\n    tmp1 = libdevice.isnan(tmp0).to(tl.int1)\n    tmp2 = 0.0\n    tmp3 = tmp0 == tmp2\n    tmp4 = tl_math.log(tmp0)\n    tmp5 = tmp0 * tmp4\n    tmp6 = tl.where(tmp3, tmp2, tmp5)\n    tmp7 = float('nan')\n    tmp8 = tl.where(tmp1, tmp7, tmp6)\n    tmp10 = tl_math.log(tmp9)\n    tmp11 = tmp0 * tmp10\n    tmp12 = tmp8 - tmp11\n    tmp13 = tl.broadcast_to(tmp12, [RBLOCK])\n    tmp15 = triton_helpers.promote_to_tensor(tl.sum(tmp13, 0))\n    tmp16 = libdevice.isnan(tmp15).to(tl.int1)\n    tmp17 = tmp15 == tmp2\n    tmp18 = tl_math.log(tmp15)\n    tmp19 = tmp15 * tmp18\n    tmp20 = tl.where(tmp17, tmp2, tmp19)\n    tmp21 = tl.where(tmp16, tmp7, tmp20)\n    tmp23 = tmp15 / tmp22\n    tmp24 = tmp21 - tmp23\n    tmp25 = 0.001\n    tmp26 = tmp24 + tmp25\n    tmp27 = tl.broadcast_to(tmp26, [RBLOCK])\n    tmp29 = triton_helpers.promote_to_tensor(tl.sum(tmp27, 0))\n    tmp30 = 0.0078125\n    tmp31 = tmp29 * tmp30\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([1], 0, tl.int32), tmp31, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(128)](arg1_1, buf0, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg1_1\n        buf2 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        triton_per_fused__softmax_0[grid(128)](arg0_1, buf2, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg0_1\n        buf3 = empty_strided_cuda((), (), torch.float32)\n        buf4 = buf3\n        del buf3\n        triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf4,\n            arg1_1, buf0, arg0_1, buf2, 1, 512, num_warps=4, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf2\n    return buf4,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'arg1_1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 5,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    RBLOCK: tl.constexpr = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 128 * x0), None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + x0, tmp8, None)\n\n\n@triton.jit\ndef triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel):\n    XBLOCK: tl.constexpr = 1\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    tl.full([1], xoffset, tl.int32)\n    tl.full([RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[:]\n    tl.full([RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.load(in_ptr1 + r0, None)\n    tmp7 = tl.load(in_ptr2 + r0, None)\n    tmp8 = tl.load(in_ptr3 + r0, None)\n    tmp2 = tmp0 / tmp1\n    tmp3 = libdevice.isnan(tmp2).to(tl.int1)\n    tmp4 = 0.0\n    tmp5 = tmp2 == tmp4\n    tmp6 = tl_math.log(tmp2)\n    tmp9 = tmp7 / tmp8\n    tmp10 = tmp2 * tmp9\n    tmp11 = tmp0 - tmp7\n    tmp12 = tmp11 * tmp9\n    tmp13 = tmp10 + tmp12\n    tmp14 = tl.where(tmp5, tmp4, tmp13)\n    tmp15 = tl.where(tmp3, tmp4, tmp14)\n    tmp16 = tl.broadcast_to(tmp15, [RBLOCK])\n    tmp18 = triton_helpers.promote_to_tensor(tl.sum(tmp16, 0))\n    tmp19 = 0.25\n    tmp20 = tmp18 * tmp19\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([1], 0, tl.int32), tmp20, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(128)](arg1_1, buf0, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        triton_per_fused__softmax_0[grid(128)](arg0_1, buf1, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg0_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2\n        del buf2\n        triton_per_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf3,\n            arg1_1, buf0, arg0_1, buf1, 1, 512, num_warps=4, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n    return buf3,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'arg1_1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 6,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, out_ptr1, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp3, None)\n    tl.store(out_ptr1 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.\n    constexpr, RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    _tmp68 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex\n        tmp0 = tl.load(in_ptr0 + r0, rmask, eviction_policy='evict_last', other=0.0\n            )\n        tmp1 = tl.load(in_ptr1 + 0)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = tl.load(in_ptr2 + 0)\n        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])\n        tmp16 = tl.load(in_ptr0 + (4096 + r0), rmask, eviction_policy=\n            'evict_last', other=0.0)\n        tmp17 = tl.load(in_ptr1 + 1)\n        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])\n        tmp20 = tl.load(in_ptr2 + 1)\n        tmp21 = tl.broadcast_to(tmp20, [XBLOCK, RBLOCK])\n        tmp33 = tl.load(in_ptr0 + (8192 + r0), rmask, eviction_policy=\n            'evict_last', other=0.0)\n        tmp34 = tl.load(in_ptr1 + 2)\n        tmp35 = tl.broadcast_to(tmp34, [XBLOCK, RBLOCK])\n        tmp37 = tl.load(in_ptr2 + 2)\n        tmp38 = tl.broadcast_to(tmp37, [XBLOCK, RBLOCK])\n        tmp49 = tl.load(in_ptr0 + (12288 + r0), rmask, eviction_policy=\n            'evict_last', other=0.0)\n        tmp50 = tl.load(in_ptr1 + 3)\n        tmp51 = tl.broadcast_to(tmp50, [XBLOCK, RBLOCK])\n        tmp53 = tl.load(in_ptr2 + 3)\n        tmp54 = tl.broadcast_to(tmp53, [XBLOCK, RBLOCK])\n        tmp3 = tmp0 - tmp2\n        tmp6 = tmp3 / tmp5\n        tmp7 = tl_math.exp(tmp6)\n        tmp8 = libdevice.isnan(tmp7).to(tl.int1)\n        tmp9 = 0.0\n        tmp10 = tmp7 == tmp9\n        tmp11 = tl_math.log(tmp7)\n        tmp12 = tmp7 * tmp11\n        tmp13 = tl.where(tmp10, tmp9, tmp12)\n        tmp14 = float('nan')\n        tmp15 = tl.where(tmp8, tmp14, tmp13)\n        tmp19 = tmp16 - tmp18\n        tmp22 = tmp19 / tmp21\n        tmp23 = tl_math.exp(tmp22)\n        tmp24 = libdevice.isnan(tmp23).to(tl.int1)\n        tmp25 = tmp23 == tmp9\n        tmp26 = tl_math.log(tmp23)\n        tmp27 = tmp23 * tmp26\n        tmp28 = tl.where(tmp25, tmp9, tmp27)\n        tmp29 = tl.where(tmp24, tmp14, tmp28)\n        tmp30 = tmp15 + tmp29\n        tmp31 = tmp7 + tmp23\n        tmp32 = tmp15 / tmp31\n        tmp36 = tmp33 - tmp35\n        tmp39 = tmp36 / tmp38\n        tmp40 = libdevice.isnan(tmp39).to(tl.int1)\n        tmp41 = tmp39 == tmp9\n        tmp42 = tl_math.log(tmp39)\n        tmp43 = tmp39 * tmp42\n        tmp44 = tl.where(tmp41, tmp9, tmp43)\n        tmp45 = tl.where(tmp40, tmp14, tmp44)\n        tmp46 = tmp30 + tmp45\n        tmp47 = tmp7 + tmp39\n        tmp48 = tmp30 / tmp47\n        tmp52 = tmp49 - tmp51\n        tmp55 = tmp52 / tmp54\n        tmp56 = libdevice.isnan(tmp55).to(tl.int1)\n        tmp57 = tmp55 == tmp9\n        tmp58 = tl_math.log(tmp55)\n        tmp59 = tmp55 * tmp58\n        tmp60 = tl.where(tmp57, tmp9, tmp59)\n        tmp61 = tl.where(tmp56, tmp14, tmp60)\n        tmp62 = tmp46 + tmp61\n        tmp63 = tmp7 + tmp55\n        tmp64 = tmp46 / tmp63\n        tmp65 = tl.broadcast_to(tmp62, [XBLOCK, RBLOCK])\n        tmp67 = _tmp34 + tmp65\n        _tmp34 = tl.where(rmask, tmp67, _tmp34)\n        tl.store(out_ptr0 + tl.broadcast_to(r0, [XBLOCK, RBLOCK]), tmp7, rmask)\n        tl.store(out_ptr1 + tl.broadcast_to(r0, [XBLOCK, RBLOCK]), tmp23, rmask\n            )\n        tl.store(out_ptr2 + tl.broadcast_to(r0, [XBLOCK, RBLOCK]), tmp48, rmask\n            )\n        tl.store(out_ptr3 + tl.broadcast_to(r0, [XBLOCK, RBLOCK]), tmp55, rmask\n            )\n    tmp34 = tl.sum(_tmp34, 1)[:, None]\n    tmp68 = tl.sum(_tmp68, 1)[:, None]\n    tmp69 = 0.001953125\n    tmp70 = tmp34 * tmp69\n    tmp71 = tmp68 * tmp69\n    tmp72 = tmp70 + tmp71\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp72, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1,), (1,), torch.float32)\n        buf1 = empty_strided_cuda((1,), (1,), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg0_1, buf0, buf1, 1, 512,\n            num_warps=4, num_stages=1)\n        buf2 = empty_strided_cuda((1,), (1,), torch.float32)\n        buf3 = empty_strided_cuda((1,), (1,), torch.float32)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf2, buf3, 1, 512,\n            num_warps=4, num_stages=1)\n        buf4 = empty_strided_cuda((409",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "SyntaxError"
      },
      "compilation_error": "'(' was never closed (tmp7gtqpmdr.py, line 154)",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 7,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 128\n    RBLOCK: tl.constexpr = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 4096 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tl.store(out_ptr0 + x0, tmp10, xmask)\n\n\n@triton.jit\ndef triton_per_fused__softmax_1(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 128\n    RBLOCK: tl.constexpr = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 4096 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tl.store(out_ptr0 + x0, tmp10, xmask)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_2(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(in_ptr0 + r1, rmask, eviction_policy='evict_last', other=0.0\n            )\n        tmp1 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp8 = tl.load(in_ptr2 + r1, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp9 = tl.load(in_ptr3 + r1, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp2 = tmp0 / tmp1\n        tmp3 = libdevice.isnan(tmp2).to(tl.int1)\n        tmp4 = 0.0\n        tmp5 = tmp2 == tmp4\n        tmp6 = tl_math.log(tmp2)\n        tmp7 = tmp2 * tmp6\n        tmp10 = tmp8 / tmp9\n        tmp11 = libdevice.isnan(tmp10).to(tl.int1)\n        tmp12 = tmp10 == tmp4\n        tmp13 = tl_math.log(tmp10)\n        tmp14 = tmp10 * tmp13\n        tmp15 = tl.where(tmp12, tmp4, tmp14)\n        tmp16 = tl.where(tmp5, tmp4, tmp15)\n        tmp17 = tl.where(tmp3, tmp4, tmp16)\n        tmp18 = tmp2 * tmp17\n        tmp19 = tmp10 * tmp17\n        tmp20 = tmp18 - tmp19\n        tmp21 = tl.broadcast_to(tmp20, [XBLOCK, RBLOCK])\n        tmp23 = _tmp34 + tmp21\n        _tmp34 = tl.where(rmask, tmp23, _tmp34)\n    tmp34 = tl.sum(_tmp34, 1)[:, None]\n    tmp35 = 0.000244140625\n    tmp36 = tmp34 * tmp35\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp36, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(128)](arg1_1, buf0, 128, 4096,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        triton_per_fused__softmax_0[grid(128)](arg0_1, buf1, 128, 4096,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        del arg0_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2\n        del buf2\n        triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_2[grid(1)](buf3,\n            buf0, buf1, arg2_1, arg3_1, 1, 4096, XBLOCK=1, RBLOCK=2048,\n            num_warps=16, num_stages=1)\n        del arg2_1\n        del arg3_1\n        del buf0\n        del buf1\n    return buf3,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1, input_2, input_3):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        arg2_1 = input_2\n        arg3_1 = input_3\n        output = call([arg0_1, arg1_1, arg2_1, arg3_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "ModelNew.forward() missing 2 required positional arguments: 'input_2' and 'input_3'",
            "error_type": "TypeError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 8,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 128\n    RBLOCK: tl.constexpr = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r1 = rindex\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (r1 + 4096 * x0), xmask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = tl.where(xmask, tmp1, float('-inf'))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])\n    tmp9 = tl.where(xmask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tl.store(out_ptr0 + x0, tmp10, xmask)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.\n    constexpr, RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r2 = rindex\n        r1 = rindex // 4096\n        tmp0 = tl.load(in_ptr0 + r2, rmask, eviction_policy='evict_first',\n            other=0.0)\n        tmp1 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp5 = tl.load(in_ptr2 + r2, rmask, eviction_policy='evict_first',\n            other=0.0)\n        tmp6 = tl.load(in_ptr3 + r1, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp2 = tmp0 / tmp1\n        tmp3 = libdevice.isnan(tmp2).to(tl.int1)\n        tmp4 = tmp2 == tmp3\n        tmp7 = tmp5 / tmp6\n        tmp8 = tmp7 == tmp3\n        tmp9 = tmp7 - tmp7\n        tmp10 = tmp9 * tmp7\n        tmp11 = tl.where(tmp8, tmp9, tmp10)\n        tmp12 = tmp2 - tmp7\n        tmp13 = tmp12 * tmp12\n        tmp14 = tl.where(tmp4, tmp9, tmp13)\n        tmp15 = tmp11 - tmp14\n        tmp16 = tl_math.exp(tmp15)\n        tmp17 = tl.where(tmp4, tmp7, tmp16)\n        tmp18 = tmp2 * tmp17\n        tmp19 = tmp7 * tmp17\n        tmp20 = tmp18 - tmp19\n        tmp21 = tl.broadcast_to(tmp20, [XBLOCK, RBLOCK])\n        tmp23 = _tmp34 + tmp21\n        _tmp34 = tl.where(rmask, tmp23, _tmp34)\n    tmp34 = tl.sum(_tmp34, 1)[:, None]\n    tmp35 = 0.000244140625\n    tmp36 = tmp34 * tmp35\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp36, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(128)](arg1_1, buf0, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((128, 1), (1, 128), torch.float32)\n        triton_per_fused__softmax_0[grid(128)](arg0_1, buf1, 128, 4096,\n            XBLOCK=8, num_warps=32, num_stages=1)\n        del arg0_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2\n        del buf2\n        triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf3,\n            arg1_1, buf0, arg0_1, buf1, 1, 4096, XBLOCK=1, RBLOCK=2048,\n            num_warps=16, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n    return buf3,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "cannot access local variable 'arg1_1' where it is not associated with a value",
            "error_type": "UnboundLocalError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 9,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": NaN,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_per_fused__softmax_0(in_ptr0, out_ptr0, out_ptr1, xnumel, rnumel,\n    XBLOCK: tl.constexpr):\n    RBLOCK: tl.constexpr = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    r0 = rindex\n    tmp0 = tl.load(in_ptr0 + r0, None)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])\n    tmp3 = triton_helpers.max2(tmp1, 1)[:, None]\n    tmp4 = tmp0 - tmp3\n    tmp5 = tl_math.exp(tmp4)\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])\n    tmp8 = tl.sum(tmp6, 1)[:, None]\n    tl.store(out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp3, None)\n    tl.store(out_ptr1 + tl.full([XBLOCK, 1], 0, tl.int32), tmp8, None)\n\n\n@triton.jit\ndef triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1(in_out_ptr0,\n    in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, rnumel, XBLOCK: tl.\n    constexpr, RBLOCK: tl.constexpr):\n    rnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex\n        tmp0 = tl.load(in_ptr0 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp1 = tl.load(in_ptr1 + 0)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = tl.load(in_ptr2 + 0)\n        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])\n        tmp15 = tl.load(in_ptr3 + r0, rmask, eviction_policy='evict_last',\n            other=0.0)\n        tmp3 = tmp0 - tmp2\n        tmp6 = tmp3 / tmp5\n        tmp7 = libdevice.isnan(tmp6).to(tl.int1)\n        tmp8 = 0.0\n        tmp9 = tmp6 == tmp8\n        tmp10 = tl_math.log(tmp6)\n        tmp11 = tmp6 * tmp10\n        tmp12 = tl.where(tmp9, tmp8, tmp11)\n        tmp13 = float('nan')\n        tmp14 = tl.where(tmp7, tmp13, tmp12)\n        tmp16 = tmp6 * tmp15\n        tmp17 = tmp14 - tmp16\n        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])\n        tmp20 = _tmp34 + tmp18\n        _tmp34 = tl.where(rmask, tmp20, _tmp34)\n    tmp34 = tl.sum(_tmp34, 1)[:, None]\n    tmp35 = 0.000244140625\n    tmp36 = tmp34 * tmp35\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp36, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (128, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (128, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1,), (1,), torch.float32)\n        buf1 = empty_strided_cuda((1,), (1,), torch.float32)\n        get_raw_stream(0)\n        triton_per_fused__softmax_0[grid(1)](arg1_1, buf0, buf1, 1, 512,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        buf2 = empty_strided_cuda((1,), (1,), torch.float32)\n        buf3 = empty_strided_cuda((1,), (1,), torch.float32)\n        triton_per_fused__softmax_0[grid(1)](arg0_1, buf2, buf3, 1, 512,\n            XBLOCK=1, num_warps=2, num_stages=1)\n        buf4 = empty_strided_cuda((), (), torch.float32)\n        buf5 = buf4\n        del buf4\n        triton_red_fused__softmax_add_div_log_mul_sub_sum_xlogy_1[grid(1)](buf5,\n            arg1_1, buf0, buf1, arg0_1, buf2, buf3, 1, 4096, XBLOCK=1,\n            RBLOCK=2048, num_warps=16, num_stages=1)\n        del arg0_1\n        del arg1_1\n        del buf0\n        del buf1\n        del buf2\n        del buf3\n    return buf5,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "inf"
        ],
        "avg_differences": [
          "inf"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "status": "runtime_error",
            "error": "dynamic_func() got multiple values for argument 'XBLOCK'",
            "error_type": "TypeError"
          }
        ],
        "pass_count": 0,
        "total_trials": 1,
        "correctness_summary": "(0 / 1)",
        "overall_max_diff": Infinity,
        "overall_avg_diff": NaN
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    }
  ]
}