{
  "name": "batch_decode_with_paged_kv_cache",
  "type": "decode",
  "description": "Performs batched decoding using a paged KV cache, sgl + FlashInfer backend.",
  "axes": {
    "B": { "type": "var" },
    "H_qo": { "type": "const", "value": 32 },
    "H_kv": { "type": "const", "value": 8 },
    "D": { "type": "const", "value": 128 },
    "T_kv": { "type": "var" },
    "I": { "type": "var" },
    "NNZ": { "type": "var" }
  },
  "constraints": [
    "H_qo == H_kv * 4",
    "I == B + 1",
    "NNZ == kv_indptr[-1]"
  ],
  "inputs": {
    "q": {
      "shape": ["B", "H_qo", "D"],
      "dtype": "bfloat16"
    },
    "k": {
      "shape": ["T_kv", "H_kv", "D"],
      "dtype": "bfloat16"
    },
    "v": {
      "shape": ["T_kv", "H_kv", "D"],
      "dtype": "bfloat16"
    },
    "kv_indptr": {
      "shape": ["I"],
      "dtype": "int32"
    },
    "kv_indices": {
      "shape": ["NNZ"],
      "dtype": "int32"
    },
    "kv_last_page_len": {
      "shape": ["B"],
      "dtype": "int32"
    },
    "sm_scale": {
      "shape": [],
      "dtype": "float32"
    },
    "logits_soft_cap": {
      "shape": [],
      "dtype": "float32"
    }
  },
  "outputs": {
    "o": {
      "shape": ["B", "H_qo", "D"],
      "dtype": "bfloat16"
    }
  },
  "reference": "import torch\n\ndef run(q, k_cache, v_cache, kv_indptr, kv_indices, kv_last_page_len, page_size, sm_scale):\n    batch_size, num_qo_heads, head_dim = q.shape\n    num_kv_heads = k_cache.shape[2]\n    \n    output = torch.empty_like(q)\n\n    for seq_idx in range(batch_size):\n        page_start = kv_indptr[seq_idx].item()\n        page_end = kv_indptr[seq_idx + 1].item()\n        seq_pages = kv_indices[page_start:page_end]\n\n        last_page_len = kv_last_page_len[seq_idx].item()\n        seq_len = (len(seq_pages) - 1) * page_size + last_page_len\n\n        # Gather keys and values\n        seq_k_pages = k_cache[seq_pages]  # (num_pages, page_size, num_kv_heads, head_dim)\n        seq_v_pages = v_cache[seq_pages]\n\n        k_seq = seq_k_pages.reshape(-1, num_kv_heads, head_dim)[:seq_len]  # (L, num_kv_heads, D)\n        v_seq = seq_v_pages.reshape(-1, num_kv_heads, head_dim)[:seq_len]\n        q_seq = q[seq_idx]  # (num_qo_heads, D)\n\n        if num_qo_heads != num_kv_heads:\n            kv_head_ratio = num_qo_heads // num_kv_heads\n            k_seq = k_seq.repeat_interleave(kv_head_ratio, dim=1)\n            v_seq = v_seq.repeat_interleave(kv_head_ratio, dim=1)\n\n        # Compute attention scores\n        attn_scores = torch.einsum(\"hd,ldh->hl\", q_seq, k_seq) * sm_scale  # (H, L)\n        attn_probs = torch.softmax(attn_scores, dim=-1)  # (H, L)\n\n        # Weighted sum\n        out = torch.einsum(\"hl,ldh->hd\", attn_probs, v_seq)  # (H, D)\n        output[seq_idx] = out\n\n    return {\"out\": output}"
}
