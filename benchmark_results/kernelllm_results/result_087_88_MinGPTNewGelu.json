{
  "device_info": {
    "device_name": "NVIDIA GeForce RTX 4090",
    "device_id": 0,
    "device_str": "cuda:0",
    "compute_capability": [
      8,
      9
    ],
    "total_memory": 25386352640,
    "driver_version": "12.4",
    "backend": "triton"
  },
  "kernel_spec": {
    "description_file": "benchmark/level1/88_MinGPTNewGelu.py",
    "generator_file": "generators/kernelllm_generator.py",
    "warmup": 5,
    "iterations": 10,
    "max_diff_limit": 1e-05,
    "report_n": 10,
    "correctness_trials": 1,
    "seed": 42,
    "backend": "triton",
    "use_ncu": false
  },
  "baseline_time": 0.08068479895591736,
  "success_rate": 1.0,
  "avg_speedup": 3.4414206633331625,
  "max_speedup": 4.804496967788981,
  "compilation_success_rate": 1.0,
  "correctness_success_rate": 1.0,
  "generations": [
    {
      "round": 0,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.01966080069541931,
      "speedup": 4.10384094757218,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 1,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.016896000504493712,
      "speedup": 4.775378583497212,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 2,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.017507199943065644,
      "speedup": 4.60866381936053,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=1024, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 3,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.01679359972476959,
      "speedup": 4.804496967788981,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 4,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.031119999289512635,
      "speedup": 2.5926992544343643,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 5,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.02991360127925873,
      "speedup": 2.6972612960466913,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=1024, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 6,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.029359999299049377,
      "speedup": 2.748119921056326,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=1024, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 7,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.029900801181793214,
      "speedup": 2.6984159543205433,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 8,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.029900801181793214,
      "speedup": 2.6984159543205433,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=1024, num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 9,
      "compiled": true,
      "correctness": true,
      "max_diff": 2.384185791015625e-07,
      "avg_diff": 3.0906671666386387e-10,
      "runtime": 0.030028799176216127,
      "speedup": 2.686913934934254,
      "generated_code": "```\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_mul_pow_tanh_0(in_ptr0, out_ptr0, xnumel, XBLOCK:\n    tl.constexpr):\n    xnumel = 4000000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = 0.5\n    tmp2 = tmp0 * tmp1\n    tmp3 = tmp0 * tmp0\n    tmp4 = tmp3 * tmp0\n    tmp5 = 0.044715\n    tmp6 = tmp4 * tmp5\n    tmp7 = tmp0 + tmp6\n    tmp8 = 0.7978845608028654\n    tmp9 = tmp7 * tmp8\n    tmp10 = libdevice.tanh(tmp9)\n    tmp11 = 1.0\n    tmp12 = tmp10 + tmp11\n    tmp13 = tmp2 * tmp12\n    tl.store(out_ptr0 + x0, tmp13, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (2000, 2000), (2000, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((2000, 2000), (2000, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_mul_pow_tanh_0[grid(4000000)](arg0_1, buf0, \n            4000000, XBLOCK=512, num_warps=8, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "correctness_trials_attempted": 1,
        "max_differences": [
          "0.000000"
        ],
        "avg_differences": [
          "0.000000"
        ],
        "trial_results": [
          {
            "trial": 0,
            "seed": 1608637542,
            "original_output_type": "<class 'torch.Tensor'>",
            "custom_output_type": "<class 'torch.Tensor'>",
            "original_output_shape": "torch.Size([2000, 2000])",
            "original_output_dtype": "torch.float32",
            "custom_output_shape": "torch.Size([2000, 2000])",
            "custom_output_dtype": "torch.float32",
            "status": "pass",
            "max_diff": 2.384185791015625e-07,
            "avg_diff": 3.0906671666386387e-10
          }
        ],
        "pass_count": 1,
        "total_trials": 1,
        "correctness_summary": "(1 / 1)",
        "overall_max_diff": 2.384185791015625e-07,
        "overall_avg_diff": 3.0906671666386387e-10
      },
      "compilation_error": null,
      "runtime_error": null,
      "generation_error": null
    }
  ]
}