{
  "name": "llama3.1_8b_flashinfer_decode_attn_paged_kv",
  "type": "attention",
  "description": "Paged KV-cache decode attention.",
  "axes": {
    "B": { "type": "var" },
    "H_q": { "type": "const", "value": 32 },
    "H_kv": { "type": "const", "value": 8 },
    "D": { "type": "const", "value": 128 },
    "T_kv": { "type": "var", "parent": "B" }
  },
  "inputs": {
    "q": {
      "shape": ["B", "H_q", "D"],
      "dtype": "bfloat16"
    },
    "k": {
      "shape": ["T_kv", "H_kv", "D"],
      "dtype": "bfloat16"
    },
    "v": {
      "shape": ["T_kv", "H_kv", "D"],
      "dtype": "bfloat16"
    },
    "sm_scale": {
      "shape": [],
      "dtype": "float32"
    },
    "logits_soft_cap": {
      "shape": [],
      "dtype": "float32"
    }
  },
  "outputs": {
    "o": {
      "shape": ["B", "H_q", "D"],
      "dtype": "bfloat16"
    }
  },
  "reference": ""
}
