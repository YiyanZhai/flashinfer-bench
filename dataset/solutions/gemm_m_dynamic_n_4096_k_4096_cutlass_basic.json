{
    "name": "cutlass_basic_gemm",
    "definition": "gemm",
    "description": "Basic matrix‑multiply (GEMM) implementation using NVIDIA CUTLASS’s Python interface (v4.1.0).",
    "author": "flashinfer-community",
    "spec": {
      "language": "Python",
      "target_hardware": [
        "NVIDIA_H100",
        "NVIDIA_A100",
        "NVIDIA_B200"
      ],
      "dependencies": [
        "cutlass >= 4.1.0",
        "torch >= 2.1"
      ],
      "entry_point": "run",
      "build_steps": []
    },
    "sources": [
      {
        "path": "main.py",
        "content": "import torch\nimport cutlass\nfrom cutlass import GemmCoord, DataType, OperationClass, layout, arch\nfrom cutlass.gemm.device import Gemm\nfrom cutlass.gemm import GemmArguments\n\n\ndef run(A: torch.Tensor, B: torch.Tensor):\n    \"\"\"\n    Launches a CUTLASS GEMM: C = A @ B\n\n    Args:\n        A: Tensor of shape (M, K)\n        B: Tensor of shape (K, N)\n    Returns:\n        dict with key \"C\" for the output tensor (M, N)\n    \"\"\"\n    # Shapes\n    M, K = A.shape\n    K2, N = B.shape\n    assert K == K2, \"Inner dimensions must match\"\n\n    # Allocate output\n    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n    # Define problem size\n    problem_size = GemmCoord(M, N, K)\n\n    # Configure the GEMM operation\n    operation = Gemm(\n        DataType.f32, layout.RowMajor,\n        DataType.f32, layout.ColumnMajor,\n        DataType.f32, layout.RowMajor,\n        DataType.f32,\n        OperationClass.TensorOp,\n        arch.SM80\n    )\n\n    # Pack arguments\n    args = GemmArguments(\n        problem_size,\n        A.data_ptr(), B.data_ptr(), C.data_ptr(),\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        1.0, 0.0\n    )\n\n    # Run!\n    operation.run(args)\n\n    return {\"C\": C}\n"
      }
    ]
  }
  