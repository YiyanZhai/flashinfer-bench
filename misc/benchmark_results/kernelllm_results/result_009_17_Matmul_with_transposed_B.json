{
  "device_info": {
    "device_name": "NVIDIA GeForce RTX 4090",
    "device_id": 0,
    "device_str": "cuda:0",
    "compute_capability": [
      8,
      9
    ],
    "total_memory": 25386352640,
    "driver_version": "12.4",
    "backend": "triton"
  },
  "kernel_spec": {
    "description_file": "benchmark/level1/17_Matmul_with_transposed_B.py",
    "generator_file": "generators/kernelllm_generator.py",
    "warmup": 5,
    "iterations": 10,
    "max_diff_limit": 1e-05,
    "report_n": 10,
    "correctness_trials": 1,
    "seed": 42,
    "backend": "triton",
    "use_ncu": false
  },
  "baseline_time": 0.3893280029296875,
  "success_rate": 0.0,
  "avg_speedup": 0.0,
  "max_speedup": 0.0,
  "compilation_success_rate": 0.1,
  "correctness_success_rate": 0.0,
  "generations": [
    {
      "round": 0,
      "compiled": true,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    ynumel = 128\n    xnumel = 4096\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    ymask = yindex < ynumel\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    x2 = xindex\n    y0 = yindex % 128\n    y1 = yindex // 128\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 128 * x2 + 524288 * y1), xmask & ymask,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, xmask & ymask)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(128, 4096)](arg0_1, buf0, 128, 4096,\n            XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg1_1, reinterpret_tensor(buf0, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg1_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "runtime_error_name": "RuntimeError"
      },
      "compilation_error": null,
      "runtime_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "generation_error": null
    },
    {
      "round": 1,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    ynumel = 1024\n    xnumel = 4096\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    ymask = yindex < ynumel\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    x2 = xindex\n    y0 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 1024 * x2), xmask & ymask,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y0), tmp0, xmask & ymask)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (1024, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (2048, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(1024, 4096)](arg0_1, buf0, 1024, 4096,\n            XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(buf0, reinterpret_tensor(arg1_1, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg1_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 2,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg0_1, buf0, \n            4194304, 4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg1_1, reinterpret_tensor(buf0, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg1_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 3,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg0_1, buf0, \n            4194304, 4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg1_1, reinterpret_tensor(buf0, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg1_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 4,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096, 4096), (16777216, 4096, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg1_1, buf0, 4194304,\n            4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((1024, 4096, 4096), (16777216, 4096, 1),\n            torch.float32)\n        extern_kernels.bmm(buf0, reinterpret_tensor(arg0_1, (1024, 4096, \n            4096), (16777216, 1, 4096), 0), out=buf1)\n        del arg0_1\n        del buf0\n    return reinterpret_tensor(buf1, (1024, 4096, 4096), (16777216, 1, 4096), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 5,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (4096, 2048), (2048, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg0_1, buf0, \n            4194304, 4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg1_1, reinterpret_tensor(buf0, (4096, 2048), (\n            1, 4096), 0), out=buf1)\n        del arg1_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 6,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096, 4096), (16777216, 4096, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg1_1, buf0, 4194304,\n            4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((1024, 4096, 4096), (16777216, 4096, 1),\n            torch.float32)\n        extern_kernels.bmm(reinterpret_tensor(arg0_1, (1024, 4096, 4096), (\n            16777216, 1, 4096), 0), buf0, out=buf1)\n        del arg0_1\n        del buf0\n    return reinterpret_tensor(buf1, (1024, 4096, 4096), (16777216, 1, 4096), 0),\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 7,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (1024, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (2048, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg1_1, buf0, \n            4194304, 4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg0_1, reinterpret_tensor(buf0, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg0_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 8,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg1_1, reinterpret_tensor(arg0_1, (4096, 2048),\n            (1, 4096), 0), out=buf0)\n        del arg0_1\n        del arg1_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](buf0, buf1, 4194304,\n            4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    },
    {
      "round": 9,
      "compiled": false,
      "correctness": false,
      "max_diff": Infinity,
      "avg_diff": Infinity,
      "runtime": null,
      "speedup": null,
      "generated_code": "```\nimport torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_clone_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK: tl.\n    constexpr, XBLOCK: tl.constexpr):\n    yoffset = tl.program_id(1) * YBLOCK\n    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, YBLOCK], True, tl.int1)\n    x2 = xindex\n    y0 = yindex % 4096\n    y1 = yindex // 4096\n    y3 = yindex\n    tmp0 = tl.load(in_ptr0 + (y0 + 4096 * x2 + 16777216 * y1), None,\n        eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x2 + 4096 * y3), tmp0, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (2048, 4096), (4096, 1))\n    assert_size_stride(arg1_1, (1024, 4096), (4096, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((1024, 4096), (4096, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_clone_0[grid(4194304, 4096)](arg1_1, buf0, \n            4194304, 4096, XBLOCK=32, YBLOCK=32, num_warps=4, num_stages=1)\n        del arg1_1\n        buf1 = empty_strided_cuda((1024, 2048), (2048, 1), torch.float32)\n        extern_kernels.mm(arg0_1, reinterpret_tensor(buf0, (4096, 2048), (1,\n            4096), 0), out=buf1)\n        del arg0_1\n        del buf0\n    return buf1,\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, input_0, input_1):\n        arg1_1 = input_0\n        arg0_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n```",
      "ncu_results": {},
      "metadata": {
        "compilation_error_name": "RuntimeError"
      },
      "compilation_error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "runtime_error": null,
      "generation_error": null
    }
  ]
}